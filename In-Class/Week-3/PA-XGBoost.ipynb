{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PA Gradient Boosting and XG Boost\"\n",
    "author: \"Chloe Feehan\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "execute:\n",
    "  message: false\n",
    "  warning: false\n",
    "theme: flatly\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Classifying Penguins\n",
    "\n",
    "Please review the following site for information on our dataset of interest here: https://allisonhorst.github.io/palmerpenguins (Links to an external site.)\n",
    "\n",
    "You can find the CSV file here: https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data (Links to an external site.)\n",
    "\n",
    "This is a very nice, simple dataset with which to apply clustering techniques, classification techniques, or play around with different visualization methods. Your goal is to use the other variables in the measurement variables in the dataset to predict (classify) species.\n",
    "\n",
    "Assignment Specs\n",
    "\n",
    "You should compare XGBoost or Gradient Boosting to the results of your previous AdaBoost activity.\n",
    "Based on the visualizations seen at the links above you're probably also thinking that this classification task should not be that difficult. So, a secondary goal of this assignment is to test the effects of the XGBoost (or Gradient Boosting) function arguments on the algorithm's performance. \n",
    "You should explore at least 3 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how the behave.\n",
    "Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways.\n",
    "Again, submit an HTML, ipynb, or Colab link. Be sure to rerun your entire notebook fresh before submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"penguins_size.csv\")\n",
    "penguins.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "penguins['species'] = le.fit_transform(penguins['species'])  # 0 = Adelie, 1 = Chinstrap, 2 = Gentoo\n",
    "\n",
    "# Encode categorical features\n",
    "penguins['sex'] = le.fit_transform(penguins['sex'])\n",
    "penguins['island'] = le.fit_transform(penguins['island'])\n",
    "\n",
    "# Define X and y\n",
    "X = penguins.drop(\"species\", axis=1)\n",
    "y = penguins[\"species\"]\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Biscoe       1.00      1.00      1.00        31\n",
      "       Dream       1.00      1.00      1.00        13\n",
      "   Torgersen       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        67\n",
      "   macro avg       1.00      1.00      1.00        67\n",
      "weighted avg       1.00      1.00      1.00        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "print(\"Gradient Boosting Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_gb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(\"XGBoost Classifier Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
