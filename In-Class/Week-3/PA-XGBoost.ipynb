{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"PA Gradient Boosting and XG Boost\"\n",
    "author: \"Chloe Feehan\"\n",
    "format: \n",
    "  html:\n",
    "    code-fold: true\n",
    "    embed-resources: true\n",
    "    toc: true\n",
    "execute:\n",
    "  message: false\n",
    "  warning: false\n",
    "theme: flatly\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Classifying Penguins\n",
    "\n",
    "Please review the following site for information on our dataset of interest here: https://allisonhorst.github.io/palmerpenguins (Links to an external site.)\n",
    "\n",
    "You can find the CSV file here: https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data (Links to an external site.)\n",
    "\n",
    "This is a very nice, simple dataset with which to apply clustering techniques, classification techniques, or play around with different visualization methods. Your goal is to use the other variables in the measurement variables in the dataset to predict (classify) species.\n",
    "\n",
    "Assignment Specs\n",
    "\n",
    "You should compare XGBoost or Gradient Boosting to the results of your previous AdaBoost activity.\n",
    "Based on the visualizations seen at the links above you're probably also thinking that this classification task should not be that difficult. So, a secondary goal of this assignment is to test the effects of the XGBoost (or Gradient Boosting) function arguments on the algorithm's performance. \n",
    "You should explore at least 3 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how the behave.\n",
    "Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways.\n",
    "Again, submit an HTML, ipynb, or Colab link. Be sure to rerun your entire notebook fresh before submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"penguins_size.csv\")\n",
    "penguins.dropna(inplace=True)\n",
    "\n",
    "# define x and y \n",
    "X = penguins.drop(\"species\", axis=1)\n",
    "y = penguins[\"species\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       1.00      1.00      1.00        31\n",
      "   Chinstrap       1.00      1.00      1.00        13\n",
      "      Gentoo       1.00      1.00      1.00        23\n",
      "\n",
      "    accuracy                           1.00        67\n",
      "   macro avg       1.00      1.00      1.00        67\n",
      "weighted avg       1.00      1.00      1.00        67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), make_column_selector(dtype_include=object)),\n",
    "        (\"standardize\", StandardScaler(), make_column_selector(dtype_include=np.number))\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "ada_pipeline = Pipeline([\n",
    "    ('preprocess', ct),\n",
    "    ('clf', AdaBoostClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "ada_pipeline.fit(X_train, y_train)\n",
    "y_pred_ada = ada_pipeline.predict(X_test)\n",
    "\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
    "print(classification_report(y_test, y_pred_ada))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.9801980198019802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.98      0.98      0.98        49\n",
      "   Chinstrap       0.94      0.94      0.94        18\n",
      "      Gentoo       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.98       101\n",
      "   macro avg       0.97      0.97      0.97       101\n",
      "weighted avg       0.98      0.98      0.98       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb_pipeline = Pipeline([\n",
    "    ('preprocess', ct),\n",
    "    ('clf', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "y_pred_gb = gb_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy (50 estimators, LR=1.0): 99.01%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adelie       0.98      1.00      0.99        49\n",
      "   Chinstrap       1.00      0.94      0.97        18\n",
      "      Gentoo       1.00      1.00      1.00        34\n",
      "\n",
      "    accuracy                           0.99       101\n",
      "   macro avg       0.99      0.98      0.99       101\n",
      "weighted avg       0.99      0.99      0.99       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chloefeehan/Desktop/python/GSB-545/gsb545_3.11/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\"dummify\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'), make_column_selector(dtype_include=object)),\n",
    "        (\"standardize\", StandardScaler(), make_column_selector(dtype_include=np.number))\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# Create the AdaBoost model with DecisionTreeClassifier as the base estimator\n",
    "model1 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', ct),\n",
    "    ('clf', model1)\n",
    "])\n",
    "\n",
    "# Fit the model and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred1 = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc1 = accuracy_score(y_test, pred1)\n",
    "\n",
    "print(f\"Model 1 Accuracy (50 estimators, LR=1.0): {acc1 * 100:.2f}%\")\n",
    "print(classification_report(y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit concerning that AdaBoost achieved 100% accuracy, as this might suggest overfitting or that the model is too simple for the data. While the other models, like Gradient Boosting and AdaBoost with Decision Trees, performed slightly lower at 98% and 99%, itâ€™s worth looking closer at why AdaBoost performed so perfectly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
